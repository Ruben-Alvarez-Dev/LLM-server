# Checkpoint Log — Llama Build Bootstrap

Date: 2025-09-06 08:35:00
Checkpoint: 2.3.0 — Programmatic llama.cpp bootstrap + build attempt

¿Qué se hizo?
- Código: `llm_server/bootstrap.py` con `ensure_llama_built()` (clone + cmake + build con Metal).
- Integración: llamada al bootstrap en `llm_server/main.py` antes de levantar el servidor.
- Docs: `docs/context/limits.md` actualizado con `gen_defaults` y overrides por ENV.

Resultado del build ahora
- Intento de `make llama.setup && make llama.build` falló: falta `cmake` en el sistema.

Cómo resolver
- Instalar dependencias locales (macOS):
  - `xcode-select --install` (si no está)
  - `brew install cmake`
- Reintentar build:
  - `make llama.clone && make llama.setup && make llama.build && make llama.test`
- Alternativa: dejar que el arranque llame al bootstrap (requiere cmake igualmente).

Próximos pasos
- Cuando tengamos `llama-cli`, avanzamos a Chapter 03 (registry/loader y speculative hooks).

