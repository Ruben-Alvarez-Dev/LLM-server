# Checkpoint Log — Env Ready & Llama Built

Date: 2025-09-06 08:50:00
Checkpoint: 2.4.0 — Dependencies installed and llama.cpp built (Metal)

¿Qué se hizo?
- Entorno Python local `.venv` con `fastapi`, `uvicorn`, y `cmake` instalado.
- Compilado `vendor/llama.cpp` con `-DGGML_METAL=ON` + build Release.
- Verificado binario: `vendor/llama.cpp/build/bin/llama-cli -h` OK.
- Verificado app: `create_app()` con FastAPI y `gen_defaults` cargados.
- Verificado acceso HF con token: HEAD 200 a un modelo planificado.

Comandos clave (ejecutados)
- `python3 -m venv .venv && pip install cmake fastapi uvicorn`
- `cmake -S vendor/llama.cpp -B vendor/llama.cpp/build -DGGML_METAL=ON -DCMAKE_BUILD_TYPE=Release`
- `cmake --build vendor/llama.cpp/build --config Release -j`
- `llama-cli -h` (OK)
- HEAD HuggingFace (200)

Estado y próximos pasos
- Infra lista. Siguiente: Chapter 03 — model registry/loader con llama.cpp y speculative decoding hooks.

